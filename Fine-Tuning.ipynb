{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Wav2vec 2.0\n",
    "\n",
    "Fine-Tuning utilizado para a iniciação científica com o objetivo de \"Estudo dos Impactos das Características dos Locutores no Desempenho de Modelos de Reconhecimento Automático de Fala\" com foco na língua portuguesa do Brasil.\n",
    "\n",
    "Autora: Lara Ramos Linhares\n",
    "\n",
    "Orientador: Luiz Henrique Merschmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As bibliotecas utilizadas neste código estão em um arquivo requirements.txt.\n",
    "\n",
    "Para fazer o download das bibliotecas utilize o comando pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando os dados para o fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from num2fawords import words, ordinal_words\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A biblioteca os é utilizada para facilitar o acesso aos arquivos de transcrição em formato csv necessários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "csv_path = os.path.join(cwd, \"BraccentPorSotaque\", \"Baiano\")\n",
    "csv_path_treino = os.path.join(csv_path, \"BaianoTreino.csv\")\n",
    "csv_path_teste = os.path.join(csv_path, \"BaianoTeste.csv\")\n",
    "\n",
    "print(\"Caminho do arquivo de treino:\", csv_path_treino)\n",
    "print(\"Caminho do arquivo de teste:\", csv_path_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É realizado o carregamento do conjunto de dados de treino e teste a partir do arquivo CSV usando load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = load_dataset(\"csv\", data_files={\"train\": csv_path_treino}, delimiter=\",\", encoding='utf8')[\"train\"]\n",
    "common_voice_test = load_dataset(\"csv\", data_files={\"test\": csv_path_teste}, delimiter=\",\", encoding='utf8')[\"test\"]\n",
    "print(common_voice_train)\n",
    "print(common_voice_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função extract_all_chars é responsável por extrair todos os caracteres únicos do conjunto de dados para a criação de um vocabulário.\n",
    "\n",
    "Algumas considerações:\n",
    "\n",
    "- 'all_text' é uma string que contém todas as sentenças concatenadas do lote. Isso é feito usando join para unir as sentenças com um espaço entre elas.\n",
    "- 'vocab' é uma lista contendo todos os caracteres únicos presentes na string 'all_text'. Isso é alcançado convertendo a string para um conjunto (set) para garantir a unicidade dos caracteres e, em seguida, convertendo de volta para uma lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"sentence\"])\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_train = common_voice_train.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_train.column_names)\n",
    "vocab_test = common_voice_train.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_test.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(sorted(set(vocab_train[\"vocab\"][0]) ))\n",
    "vocab_list = [vocab for vocab in vocab_list if vocab not in [\" \", \"\\u0307\",\"<\",\">\"]]\n",
    "print(len(vocab_list))\n",
    "print(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_vocab = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\", \"|\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {v: k for k, v in enumerate(special_vocab + vocab_list)}\n",
    "print(len(vocab_dict))\n",
    "print(vocab_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escreve o vocabulário resultante e armazenado em 'vocab_dict' em um arquivo JSON. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta parte, é determinado o modelo pré-treinado utilizado e o local em que será salvo os resultados finais. \n",
    "\n",
    "Nesse experimento, será utilizado o modelo https://huggingface.co/facebook/wav2vec2-large-xlsr-53 que é um dos muitos modelos multilíngues disponíveis no HuggingFace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer, AutoProcessor, AutoModelForPreTraining\n",
    "\n",
    "save_dir = \"./novo-treinamento/\"\n",
    "model_name_or_path = \"facebook/wav2vec2-large-xlsr-53\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este trecho de código verifica se existem checkpoints salvos no diretório 'save_dir' e, se existirem, recupera o caminho do último checkpoint usando a função 'get_last_checkpoint' do módulo transformers.trainer_utils. \n",
    "\n",
    "Isso pode ser útil, por exemplo, ao retomar o treinamento de um modelo a partir do ponto em que parou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "last_checkpoint = None\n",
    "if os.path.exists(save_dir):\n",
    "    last_checkpoint = get_last_checkpoint(save_dir)\n",
    "\n",
    "print(last_checkpoint if last_checkpoint else str(None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criação de um novo tokenizer Wav2Vec2CTCTokenizer usando um arquivo JSON específico como vocabulário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\n",
    "        \"./vocab.json\", \n",
    "        bos_token=\"<s>\",\n",
    "        eos_token=\"</s>\",\n",
    "        unk_token=\"<unk>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        word_delimiter_token=\"|\",\n",
    "        do_lower_case=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Eu rasguei todo meu dinheiro em vão, minha família depende da aposentadoria do meu avô. Mas o dia amanheceu ensolarado e eu fiquei feliz porque minha tia viajou para Porto Alegre às sete horas.\"\n",
    "print(\" \".join(tokenizer.tokenize(text)))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuração de um Wav2Vec2FeatureExtractor para extrair características de áudio.\n",
    "\n",
    "Este código é útil para configurar o Feature Extractor para extrair características de áudio, seja criando um novo a partir do zero ou carregando um existente de um modelo pré-treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "\n",
    "if not os.path.exists(save_dir) and not model_name_or_path:\n",
    "    print(\"Load from scratch\")\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "else:\n",
    "    print(f\"Load from {model_name_or_path}\")\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este trecho de código verifica se o tamanho do vocabulário retornado pelo método get_vocab() do tokenizer é igual ao tamanho total do tokenizer. Se ambos forem iguais, o código imprime o tamanho do tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(processor.tokenizer.get_vocab()) == len(processor.tokenizer):\n",
    "    print(len(processor.tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redimencionamento dos dados (Resampling data)\n",
    "\n",
    "A taxa de amostragem na qual os arquivos de áudio devem ser digitalizados é de 16.000Hz. Como o modelo Wav2Vec2 pré-treinado é treinado com arquivos de áudio com taxa de amostragem de 16.000 Hz, devemos reamostrar nossos dados de acordo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se você deseja redimencionar o áudio para a taxa alvo (target_sampling_rate), você pode descomentar a linha #speech_array = librosa.resample(np.asarray(speech_array), sampling_rate, target_sampling_rate) e fornecer a implementação correta para resamplear usando librosa. \n",
    "\n",
    "Isso é útil se a taxa de amostragem do áudio original for diferente da target_sampling_rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "target_sampling_rate = 16000\n",
    "\n",
    "cwd = os.getcwd()\n",
    "audio_path = os.path.join(cwd, \"BraccentPorSotaque\", \"Baiano\")\n",
    "        \n",
    "def speech_file_to_array_fn(batch):\n",
    "    audio_file_path = os.path.join(audio_path, batch[\"path\"])\n",
    "    speech_array, sampling_rate = torchaudio.load(audio_file_path, normalize=True)  \n",
    "    speech_array = speech_array.squeeze().numpy()\n",
    "    #speech_array = librosa.resample(np.asarray(speech_array), sampling_rate, target_sampling_rate)\n",
    "    \n",
    "    batch[\"speech\"] = speech_array\n",
    "    batch[\"sampling_rate\"] = target_sampling_rate\n",
    "    batch[\"duration_in_seconds\"] = len(batch[\"speech\"]) / target_sampling_rate\n",
    "    batch[\"target_text\"] = batch[\"sentence\"]\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este trecho está mapeando a função 'speech_file_to_array_fn' nos conjuntos de dados de treinamento e teste, 'remove_columns' é usado para remover as colunas existentes, mantendo apenas as novas colunas adicionadas pela função.\n",
    "\n",
    "- 'num_proc' especifica o número de processos paralelos a serem utilizados para mapeamento (pode acelerar o processamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.map(speech_file_to_array_fn, remove_columns=common_voice_train.column_names, num_proc=19)\n",
    "common_voice_test = common_voice_test.map(speech_file_to_array_fn, remove_columns=common_voice_test.column_names, num_proc=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(common_voice_train[0][\"sampling_rate\"])\n",
    "print(common_voice_test[0][\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função 'filter_by_max_duration' filtra os exemplos de dados com base na duração do áudio.\n",
    "\n",
    "- 'min_duration_in_seconds' e 'max_duration_in_seconds': definem os limites inferior e superior para a duração desejada do áudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_duration_in_seconds = 5.0\n",
    "max_duration_in_seconds = 10.0\n",
    "\n",
    "def filter_by_max_duration(batch):\n",
    "   return min_duration_in_seconds <= batch[\"duration_in_seconds\"] <= max_duration_in_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Split sizes [BEFORE]: {len(common_voice_train)} train and {len(common_voice_test)} validation.\")\n",
    "\n",
    "_common_voice_train = common_voice_train.filter(filter_by_max_duration, num_proc=24)\n",
    "\n",
    "_common_voice_test = common_voice_test.filter(filter_by_max_duration, num_proc=4)\n",
    "\n",
    "print(f\"Split sizes [AFTER]: {len(_common_voice_train)} train and {len(_common_voice_test)} validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este trecho de código parece ser usado para visualizar aleatoriamente um exemplo do conjunto de treinamento 'common_voice_train'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "sample = common_voice_train\n",
    "rand_int = random.randint(0, len(sample))\n",
    "\n",
    "print(\"Target text:\", sample[rand_int][\"target_text\"])\n",
    "print(\"Input array shape:\", np.asarray(sample[rand_int][\"speech\"]).shape)\n",
    "print(\"Sampling rate:\", sample[rand_int][\"sampling_rate\"])\n",
    "\n",
    "ipd.Audio(data=np.asarray(sample[rand_int][\"speech\"]), autoplay=True, rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função 'prepare_dataset' é destinada a preparar o conjunto de dados para treinamento, processando os sinais de áudio e os textos-alvo.\n",
    "\n",
    "- 'assert len(set(batch[\"sampling_rate\"])) == 1': Verifica se todos os sinais de áudio no lote têm a mesma taxa de amostragem. Isso é importante para garantir consistência.\n",
    "\n",
    "- 'batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"][0]).input_values': Usa o 'processor' (que inclui o 'feature_extractor' e o 'tokenizer') para processar os sinais de áudio ('batch[\"speech\"]') e obter os 'input_values' que serão usados como entrada para o modelo.\n",
    "\n",
    "- 'with processor.as_target_processor():': Configura o processador Wav2Vec2 para atuar como um processador de destino.\n",
    "\n",
    "- 'batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids': Usa o processador para processar os textos-alvo '(batch[\"target_text\"])' e obter os 'input_ids' que representam as sequências de tokens.\n",
    "\n",
    "- 'return batch': Retorna o batch modificado após o processamento.\n",
    "\n",
    "Essa função é uma etapa importante na preparação dos dados para treinamento do modelo. Certifica-se de que os sinais de áudio e os textos-alvo estejam em um formato adequado para serem usados como entrada e rótulos durante o treinamento do modelo Wav2Vec2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # check that all files have the correct sampling rate\n",
    "    assert (\n",
    "        len(set(batch[\"sampling_rate\"])) == 1\n",
    "    ), f\"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.\"\n",
    "\n",
    "    batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"][0]).input_values\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_common_voice_train = _common_voice_train.map(prepare_dataset, remove_columns=_common_voice_train.column_names, batch_size=8, num_proc=19, batched=True)\n",
    "_common_voice_test = _common_voice_test.map(prepare_dataset, remove_columns=_common_voice_test.column_names, batch_size=8, num_proc=19, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa classe 'DataCollatorCTCWithPadding' é projetada para ser usada como um 'data_collator' ao treinar um modelo Wav2Vec2 para tarefas de CTC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função load_metric(\"wer\") está sendo usada para carregar a métrica de taxa de erro de palavra (Word Error Rate, WER). Usado para a avaliação do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este trecho de código define uma função 'compute_metrics' que é usada para calcular a métrica Word Error Rate (WER) com base nas predições do modelo e nos rótulos de referência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "    \n",
    "    if isinstance(label_str, list):\n",
    "        if isinstance(pred_str, list) and len(pred_str) == len(label_str):\n",
    "            for index in random.sample(range(len(label_str)), 3):\n",
    "                print(f'reference: \"{label_str[index]}\"')\n",
    "                print(f'predicted: \"{pred_str[index]}\"')\n",
    "\n",
    "        else:\n",
    "            for index in random.sample(range(len(label_str)), 3):\n",
    "                print(f'reference: \"{label_str[index]}\"')\n",
    "                print(f'predicted: \"{pred_str}\"')\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apontando as configurações do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    attention_dropout=0.1,\n",
    "    activation_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    feat_proj_dropout=0.1,\n",
    "    final_dropout=0.1,\n",
    "    layerdrop=0.1,\n",
    "    gradient_checkpointing=False,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    ctc_zero_infinity=True,\n",
    "    bos_token_id=processor.tokenizer.bos_token_id,\n",
    "    eos_token_id=processor.tokenizer.eos_token_id,\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer.get_vocab())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_dir,\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=4,  \n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=10,\n",
    "    fp16=True,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=10,\n",
    "    learning_rate=4e-4,\n",
    "    warmup_steps=500,\n",
    "    save_total_limit=2,\n",
    "    dataloader_num_workers=16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=_common_voice_train,\n",
    "    eval_dataset=_common_voice_test,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"step1\")\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"step2\")\n",
    "metrics = train_result.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"step3\")\n",
    "max_train_samples = len(_common_voice_train)\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(_common_voice_train))\n",
    "\n",
    "print(\"step4\")\n",
    "trainer.save_model()\n",
    "\n",
    "print(\"model created!\")\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_path = \"./novo-treinamento/\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ic-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
